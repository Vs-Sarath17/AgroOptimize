# -*- coding: utf-8 -*-
"""Master Copy_crop_yield_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14wuxSTd80vRQfjLTkdQT9SmGCayLF_Bo

# **Problem Statement:**:
>To achieve sustainable agriculture, it is vital to accurately predict crop yields and efficiently manage resources. This can help maximize agricultural output while reducing negative environmental effects. Predicting yields and managing resources effectively is challenging due to the complex interactions/relations of various factors such as soil quality, weather(☀️🌦️🌧️⛈️🌨️), and farming practices. To improve yield estimates and optimize resource allocation, it is essential to develop a reliable predictive model that makes use of these variables.

**Background:**
>Farming is key to the economy in India, providing food🍽️ and financial stability. Yet, traditional farming methods often rely on past experiences, which can lead to inefficient use of resources and reduced crop yields. By utilizing machine learning and data-driven approaches, it's possible to tap into historical data on crops, soil conditions, weather patterns(☀️🌦️🌧️⛈️🌨️), and farming practices. This data can be used to build predictive models that guide decisions in farming.

**Relevance:**
>Precision crop yield predictions and wise resource useages in India are important for sustainable and better farming. Farmers can limit waste, cut down on the harm they do to the environment, and make the most of their resources (fertilizer💊, water💧, labor). Also, having estimates of how much will be produced can help in planning for crops, keeping track of the supply chain, and creating policies. Giving farmers better management tools for their resources can help them cut costs and make more money💰, which can make their financial situation better overall.

**Objectives:**
>The primary objectives of this project are:

1. **Crop Yield Prediction**:
>Develop a machine learning model that can accurately predict crop yields based on various input features, including soil nutrient levels, climatic conditions (rainfall🌧️, temperature🌡️), crop type.

2. **Resource Optimization**:
>Usage of the predictive model to optimize resource usage, while considering environmental constraints and sustainability of the crops.


By addressing these objectives, our project aims to contribute to the betterment of agriculture🌾 and environmental for farming communities.

### We install kaggle to import dataset 📁📂
"""

!pip install q kaggle

"""### **Now we import the necessary libraries**

---


1.   pandas
2.   numpy
3.   seaborn
4.   matplotlib




"""

from google.colab import files
import pandas as pd
import numpy as np
from google.colab import autoviz
import seaborn as sns
import matplotlib.pyplot as plt

"""### Now we upload the kaggle api"""

files.upload()

# from google.colab import drive
# drive.mount('/content/drive')

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

"""### Now we import the dataset from **kaggle** 🌐"""

!kaggle datasets download sriharikatare/indian-crop-production

"""### Now we unzip it so that we can use it"""

!unzip indian-crop-production.zip

df=pd.read_csv('/content/Crop_production.csv')
df.info()

"""## **Data Dictonary** 📔
1.  State_Name------------------------------>Name of the state where the crop is grown          
2. Crop_Type--------------------------------->Name of the type of the crop grown (rice, wheat,lentils e.t.c)             
3. Crop------------------------------------------->Name of the crop grown                  
4. N------------------------------------------------>Amount of Nitrogen                     
5.  P------------------------------------------------>Amount of Phosphorus                     
6. K------------------------------------------------>Amount of Potassium                    
7.  pH---------------------------------------------->Acidic or Basic level of the land                    
8.  rainfall---------------------------------------->Volume of rainfall            
9.  temperature-------------------------------->Temperature in Celsius           
10. Area_in_hectares------------------------>Area of crop land in hectares     
11. Production_in_tons--------------------->Production of the yeild in tons
12. Yield_ton_per_hec----------------------->Area_in_hectares/Production_in_tons

#**Hypotheses** 🔭
### Based on the dataset, let’s create some hypotheses:
Hypothesis 1:

    Nutrient Levels (N, P, K) affect crop yield.
    Higher nutrient levels lead to increased yield.
    Specific nutrient imbalances may hinder growth.
Hypothesis 2:

    pH Levels influence crop productivity.
    Optimal pH (neither too acidic nor too basic) promotes better yields.
Hypothesis 3:

    Rainfall plays a significant role.
    Adequate rainfall positively correlates with yield.
    Drought conditions may reduce production.
Hypothesis 4:

    Temperature impacts crop growth.
    Moderate temperatures are ideal for most crops.
    Extreme heat or cold can be detrimental.
Hypothesis 5:

    Area of Land affects yield.
    Larger areas may lead to higher overall production.

Hypothesis 6:

    Different crops have varying requirements.
    Some crops thrive in specific climates or soil conditions.
"""

df.describe()

df.head()

"""### Now we are going to check for any missing values in the dataset 🔭"""

#Checking missing data values
missing_vals = df.isnull().sum()
print(missing_vals)

missing_percentage = (missing_vals/len(df) * 100)
print(missing_percentage)

"""#### We see that the dataset doesn't have any missing data/values
So we then proceed to eliminate the duplicate rows
"""

dup_rows = df.duplicated()

print("Duplicate rows: ", dup_rows)

"""### Let's find  **Z**-score for Outliers

"""

#df=pd.read_csv('/content/Crop_production.csv')
df.columns

df.info()

tempdf=df.copy(deep=True)
tempdf.drop(tempdf.columns[[0,1,2,3,10,11]],axis=1,inplace=True)

tempdf.head()

threshold = 3
z_score = np.abs(tempdf - (tempdf.mean())/ tempdf.std())
outliers = (z_score > threshold)

print("Rows containing outliers: ")
print(tempdf[outliers.any(axis=1)])

"""### **Box Plot for  detecting outliers** 📊"""

plt.figure(figsize=(10, 6))
plt.boxplot([df['N'], df['P'], df['K'], df['pH'], df['rainfall'], df['temperature'], df['Area_in_hectares'], df['Production_in_tons'], df['Yield_ton_per_hec']])
plt.xticks(ticks=range(1, 10), labels=['N','P','K','pH','rainfall','temperature','Area_in_hectares','Production_in_tons','Yield_ton_per_hec'], rotation=45)
plt.title("Box Plot of Numerical Variables")
plt.xlabel('Variables')
plt.ylabel('Values')
plt.grid(axis='y', linestyle = '--', alpha=0.7)
plt.show()

"""## **Dimensionality Reduction**
we remove the following features,

    Crop_Type
    Area_in_hectares
    Production_in_tons


---


instead of them we only need,
    
    Yield_ton_per_hec


> which is the '**Production_in_tons'/'Area_in_hectares**' value.






"""

df.columns

df.drop(['Unnamed: 0','Area_in_hectares','Production_in_tons'], axis=1, inplace=True)

df.columns

"""### Counting unique Crop season types for encoding"""

crop_season_type_count = df['Crop_Type'].value_counts()
print("Count of unique crop types: ")
print(crop_season_type_count)

# Check the unique values in the 'Crop Type' column after filtering
print("\nUnique crop season types after filtering:")
uni=df['Crop_Type'].unique()
print(uni)
print(len(uni))

# Print the number of rows and columns
print("\nNumber of rows:", df.shape[0])
print("Number of columns:", df.shape[1])

"""### Counting unique Crop types for encoding"""

crop_type_count = df['Crop'].value_counts()
print("Count of unique crop types: ")
print(crop_type_count)

# To Check the unique values in the 'Crop' column after filtering
print("\nUnique crop types after filtering:")
print(df['Crop'].unique())

print("the total number of unique crop types is:")
print(len(df['Crop'].unique()))

"""### **Pie chart for Crop_Types visualization**📊"""

# Frequency of each crop type
crop_type_count = df['Crop_Type'].value_counts()

# Plotting a pie chart
plt.figure(figsize=(8, 6))
plt.pie(crop_type_count, labels=crop_type_count.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Crop Seasons Types')
plt.axis('equal')  # Equal aspect ratio for circle
plt.show()

"""## **Plotting the crop types**"""

# Frequency of each crop type
crop_count = df['Crop'].value_counts()

# Plotting a pie chart
plt.figure(figsize=(8, 6))
plt.pie(crop_count, labels=crop_count.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Crop Types')
plt.axis('equal')  # Equal aspect ratio for circle
plt.show()

"""😅 We have 53 crop types, so the labels and the values are overlapped, we will look for other ways to visualize this."""

#this some test cell. not important, just for testing code

crop_count = df['Crop'].value_counts()
print(type(crop_count))
print(crop_count)
size=crop_count.to_numpy()
print(size)

# Frequency of each crop type
crop_count = df['Crop'].value_counts()
label=crop_count.index
sizeofeachcrop=crop_count.to_numpy()
sizeofeachcropinlist=sizeofeachcrop.tolist()
sumofsize=sum(sizeofeachcrop)
percentage=[x*100/sumofsize for x in sizeofeachcropinlist]
Label = [f'{l}, {s:0.3f}%' for l, s in zip(label, percentage)]
# Plotting a pie chart
plt.figure(figsize=(21,16))
plt.pie(crop_count, startangle=90,radius=1)
plt.legend(labels=Label,title='names')
plt.title('Distribution of Crop Types')
plt.axis('equal')
plt.show()

"""Now this looks much better

## Histogram for Crops📊
"""

plt.figure(figsize=(10, 6))
plt.hist(df['Crop'], bins=30, edgecolor='black', alpha=0.7)
plt.xlabel('Crop')
plt.ylabel('Frequency')
plt.title('Distribution of Crops')
plt.xticks(rotation=90)  # Rotatation of x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for y-axis
plt.show()

"""## Histogram for State_Names frequency📊"""

plt.figure(figsize=(10, 6))
plt.hist(df['State_Name'], bins=30, edgecolor='black', alpha=0.7)
plt.xlabel('States')
plt.ylabel('Frequency')
plt.title('States Frequency')
plt.xticks(rotation=90)  # Rotatation of x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for y-axis
plt.show()

crop_count = df['Crop'].nunique()
print("The unique types of crops: ")
print(crop_count)

state_count = df['State_Name'].nunique()
state_count

"""---



---




---



---

### we observed that the dataset has a minority class which causes class imbalances. So, we decided generate samples of that minority class using **SMOTE(Synthetic Minority Oversampling Technique)**

### now we import libraries necessary for using SMOTE
"""

from imblearn.over_sampling import SMOTE

#Applying SMOTE to the training data for Summer class only


smote = SMOTE(sampling_strategy = 'minority', random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

crop_type_count = df['Crop_Type'].value_counts()
print("Count of unique crop types: ")
print(crop_type_count)

# Check the unique values in the 'Crop Type' column after filtering
print("\nUnique crop types after filtering:")
print(df['Crop_Type'].unique())

# Print the number of rows and columns
print("\nNumber of rows:", df.shape[0])
print("Number of columns:", df.shape[1])

"""### **Looks like the SMOTE is successfully implemented**
But, SMOTE didn't generate new samples, this happens when the ratio is sufficient in compared to other with samples.

#**Bivariate Analysis 📊**
"""

plt.figure(figsize=(10,8))
sns.scatterplot(x='pH', y='rainfall', data=df, color='blue')
plt.title('pH v/s rainfall')
plt.xlabel('pH')
plt.ylabel('rainfall')
plt.plot()

"""## **Now we make the assumptions of linear regression, by Bivariate analysis**

### **We do this by checking if the variables in the dataset have linear relationship, if the variables (independent variables and the dependent variable aka the target variable) have linear relationship then we can use Linear regression.**

 we check the relationship between the variables in the dataset by using scatterplot
"""

df.keys()
#this is done to just recheck the names of the column names

"""## The dependent variable is '**Yeild_ton_per_hec'**"""

df.rename(columns={'N': 'nitrogen', 'P': 'phosphorus', 'K': 'potassium', 'Crop_Encoded':'crop name'}, inplace=True)

"""### The scatter plot of nitrogen vs dependent variable 📊"""

df.plot(kind='scatter', x='nitrogen', y='Yield_ton_per_hec', s=32, alpha=.8)

"""### The scatter plot of phosphorus vs dependent variable 📊"""

df.plot(kind='scatter', x='phosphorus', y='Yield_ton_per_hec', s=32, alpha=.8)

"""### The scatter plot of potassium vs dependent variable 📊"""

df.plot(kind='scatter', x= 'potassium', y='Yield_ton_per_hec', s=32, alpha=.8)

"""### The scatter plot of pH vs dependent variable 📊"""

df.plot(kind='scatter', x= 'pH', y='Yield_ton_per_hec', s=32, alpha=.8)

"""### The scatter plot of rainfall vs dependent variable 📊"""

df.plot(kind='scatter', x= 'rainfall', y='Yield_ton_per_hec', s=32, alpha=.8)

"""### The scatter plot of temperature vs dependent variable 📊"""

df.plot(kind='scatter', x='temperature', y='Yield_ton_per_hec', s=32, alpha=.8)

"""###Wanted to try the plotting of the above graph 📊 using different library **(Matplotlib)** instead of **(Pandas)**"""

plt.figure(figsize=(8, 6))
plt.scatter(df['temperature'], df['Yield_ton_per_hec'], color='blue', label='Data Points')
plt.title('Scatter Plot of Independent vs. Dependent Variable')
plt.xlabel('Independent Variable')
plt.ylabel('Dependent Variable')
plt.show()

"""### **We have done scatter plotting for all the independent variables and the dependent variables(target) 🎉**

We don't see much linearity in the data.

###**OUTLIERS 🥷🏻 were found in the graphs**

In the above graphs we see that the **Yield_ton_per_hec** has outliers.
So the below 2 cells are to remove the one outlier at a time and visualize it.
repeat the 2 cells until data seems to be clear of any outliers.

we have located the outlier to be at row number 83714.
"""

null_values = df['Crop_Type'].isna().sum()
print(null_values)

ndf=df.copy(deep=True)

column_name = 'Yield_ton_per_hec'
for _ in range(9):


    # Find the highest value in the specified column
    max_value = ndf[column_name].max()
    print(max_value) #run this whole cell 9 times, untill the max_value is <=300

    # Identify the row(s) with the highest value in the column
    outlier_rows = ndf[ndf[column_name] == max_value]
    print(outlier_rows)
    print(outlier_rows.index)
    # Remove the outlier row(s) from the DataFrame
    ndf = ndf.drop(outlier_rows.index).reset_index(drop=True)

    print(ndf.shape[0])

ndf.tail()

ndf.info()

null_values = ndf['Crop_Type'].isna().sum()
print(null_values)

ndf.plot(kind='scatter', x='temperature', y='Yield_ton_per_hec', s=32, alpha=.8)

ndf.columns

null_values = ndf['Crop_Type'].isnull()
filter=ndf[null_values]
# Print the boolean Series
filter.head()

"""# **Encoding categorical variables**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.preprocessing import LabelEncoder

"""### Encoding the Indian state names using one hot encoding

but before doing that we can see that there are 33 state names. and if we do one hot encoding to all of them we will have 33 extra columns. and that increases the dimensions of the dataset and we will end up facing the **curse of dimentions.** 🙅🏻‍♂️👎🏻

So what we will do is, we will group the states based on their position on the map.
"""

null_values = df['Crop_Type'].isna().sum()
print(null_values)

df=ndf.copy(deep=True)

df.tail()

null_values = df['Crop_Type'].isna()
print(null_values)

max_value = df[column_name].max()
print(max_value)

state_to_region = {
    'andaman and nicobar islands': 'South',
    'andhra pradesh': 'South',
    'arunachal pradesh': 'North',
    'assam': 'North',
    'bihar': 'East',
    'chandigarh': 'North',
    'chhattisgarh': 'East',
    'dadra and nagar haveli': 'West',
    'goa': 'West',
    'gujarat': 'West',
    'haryana': 'North',
    'himachal pradesh': 'North',
    'jammu and kashmir': 'North',
    'jharkhand': 'East',
    'karnataka': 'South',
    'kerala': 'South',
    'madhya pradesh': 'West',
    'maharashtra': 'West',
    'manipur': 'North',
    'meghalaya': 'North',
    'mizoram': 'North',
    'nagaland': 'North',
    'odisha': 'East',
    'puducherry': 'South',
    'punjab': 'North',
    'rajasthan': 'West',
    'sikkim': 'North',
    'tamil nadu': 'South',
    'telangana': 'South',
    'tripura': 'North',
    'uttar pradesh': 'North',
    'uttarakhand': 'North',
    'west bengal': 'East'
}
df['Region'] = df['State_Name'].map(state_to_region)
region_counts = df['Region'].value_counts()
print(region_counts)

df.columns

null_values = df['Region'].isna().sum()
print(null_values)
df.info()

data = df['Region']
encoder = OneHotEncoder(sparse_output=False)
data_reshaped = data.values.reshape(-1, 1)

one_hot_encoded = encoder.fit_transform(data_reshaped)

one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.categories_[0])

print(encoder.categories_)

one_hot_df.info()

df = pd.concat([df, one_hot_df], axis=1)

null_values = df['Region'].isnull()
filter=df[null_values]
# Print the boolean Series
filter.head()

df=df.drop(['Region','State_Name'], axis=1)

df.head(10)

null_values = df['Crop_Type'].isna()
print(null_values)
null_values = df['Crop_Type'].isna().sum()
print(null_values)

"""### Encoding the Crop Types using One Hot Encoding"""

data = df['Crop_Type']
encoder = OneHotEncoder(sparse_output=False)
data_reshaped = data.values.reshape(-1, 1)

one_hot_encoded = encoder.fit_transform(data_reshaped)

one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.categories_[0])

df = pd.concat([df, one_hot_df], axis=1)

one_hot_df.head()

df.head()

"""now we drop the crop_type vaiable as we have encoded it and added it back in the df"""

df=df.drop(['Crop_Type'], axis=1)

print(len(encoder.categories_))
print((encoder.categories_))

#now we make a copy
cdf=df.copy(deep=True)

"""### now we encode **Crop** variable using **one hot encoding**"""

data = df['Crop']
encoder = OneHotEncoder(sparse_output=False)
data_reshaped = data.values.reshape(-1, 1)

one_hot_encoded = encoder.fit_transform(data_reshaped)

one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.categories_[0])

print(encoder.categories_[0])
print(len(encoder.categories_))

df = pd.concat([df, one_hot_df], axis=1)

"""now we drop the **"CROP"** variable as we have one hot encoded it"""

df=df.drop(['Crop'], axis=1)

df.head()

print(type(df))
df.columns

df.shape

"""Concatenate the one-hot encoded Indian States with the features

### now we will do **label** **encoding** on **'Crop'** to the copy of the dataframe we made earlier
"""

cdf.columns

crop_encoder = LabelEncoder()
cdf['Crop_Encoded'] = crop_encoder.fit_transform(cdf['Crop'])
cdf=cdf.drop(cdf.columns[[0]], axis=1, inplace=False)

"""#**Now we move to the next step, Univariate analysis** 📊

## **Understanding the distribution of the data.**
this is done by Plotting the histograms of individual variables (both the dependent and independent)
"""

df.columns

N_data = df['nitrogen']
P_data = df['phosphorus']
K_data = df['potassium']
pH_data = df['pH']
rainfall_data = df['rainfall']
temperature_data = df['temperature']
Yield_ton_per_hec_data = df['Yield_ton_per_hec']

DF=df[['nitrogen', 'phosphorus', 'potassium', 'pH', 'rainfall', 'temperature','Yield_ton_per_hec']]
DF.hist(bins=30, figsize=(12, 10),grid=False)

plt.show()

"""The distribution is not in good shape, especially if we look at **'Yield_ton_per_hec'** distribution we can see that its very poorly distributed.

To tackle this issue we will use transformations

###The types of transformations are :-

    Log Transformation
    Square-Root Transformation
    Reciprocal Transformation
    Box-Cox Transformation

### we will go with the **log transformation** for it's simplicity and effectiveness
"""

N = np.log(N_data + 1)  # Adding 1 to avoid log(0)
P = np.log(P_data + 1)
K = np.log(K_data + 1)
pH = np.log(pH_data + 1)
rainfall = np.log(rainfall_data + 1)
temperature = np.log(temperature_data + 1)
Yield_ton_per_hec = np.log(Yield_ton_per_hec_data + 1)

new_df = pd.concat([N ,
P,
K ,
pH ,
rainfall ,
temperature,
Yield_ton_per_hec], axis=1)

new_df.columns

new_df.hist(bins=30, figsize=(12, 10),grid=False)

plt.show()

"""### **Now the Distribution is much better and spread out**"""

DF=new_df.copy(deep=True)

DF.columns

"""###Now lets do the graph plotting of a single variable to have a in-depth look at what's happening after the log transformation."""

ddf=pd.read_csv('/content/Crop_production.csv')
plt.figure(figsize=(8, 6))
sns.histplot(ddf['Yield_ton_per_hec'], bins=20, kde=True, color='skyblue')
plt.title('Histogram of Yield_ton_per_hec before the transformation')
plt.xlabel('Yield_ton_per_hec values')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(DF['Yield_ton_per_hec'], bins=20, kde=True, color='skyblue')
plt.title('Histogram of Yield_ton_per_hec after the transformation')
plt.xlabel('Yield_ton_per_hec values')
plt.ylabel('Frequency')
plt.show()

DF.head()

"""###Now Let's plot a Q-Q plot"""

import scipy.stats as stats
import matplotlib.pyplot as plt

feature_data = DF['nitrogen']
# Plotting the Q-Q plot
stats.probplot(feature_data, dist="norm", plot=plt)
plt.title('Q-Q Plot')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Ordered Values')
plt.show()

"""##**Now lets merge the transformed valriables with the original dataframe**"""

df.shape
df.columns

print(ndf.shape)
ndf.columns

print(DF.shape)
DF.columns

cdf.columns

DFOH = df[[ 'East', 'North', 'South', 'West', 'kharif', 'rabi',                 #one hot encoded
       'summer', 'whole year', 'apple', 'arecanut', 'ashgourd', 'banana',
       'barley', 'beetroot', 'bittergourd', 'blackgram', 'blackpepper',
       'bottlegourd', 'brinjal', 'cabbage', 'cardamom', 'carrot', 'cashewnuts',
       'cauliflower', 'coffee', 'coriander', 'cotton', 'cucumber', 'drumstick',
       'garlic', 'ginger', 'grapes', 'horsegram', 'jackfruit', 'jowar', 'jute',
       'ladyfinger', 'maize', 'mango', 'moong', 'onion', 'orange', 'papaya',
       'pineapple', 'pomegranate', 'potato', 'pumpkin', 'radish', 'ragi',
       'rapeseed', 'rice', 'ridgegourd', 'sesamum', 'soyabean', 'sunflower',
       'sweetpotato', 'tapioca', 'tomato', 'turmeric', 'watermelon', 'wheat']]

DFL= cdf[['East', 'North', 'South', 'West', 'kharif', 'rabi',                   #label encoded
       'summer', 'whole year', 'Crop_Encoded']]

# Concatenate along columns (axis=1)
finalOHDF = pd.concat([DF,DFOH], axis=1)
finalLDF = pd.concat([DF,DFL], axis=1)

finalOHDF.columns

finalOHDF.shape

"""# **Multivariate Analysis📊**"""

sns.pairplot(finalLDF[['nitrogen','phosphorus','potassium','pH','rainfall','temperature','Yield_ton_per_hec']], diag_kind='kde')
plt.suptitle('Pairplot of Numerical Variables', y=1.02)
plt.show()

"""### Before going with the One Hot encoding on the **Crop** names variable we tried label encoding.
But, unfortunately after the label encoding the data was not easy to visualize, we couldn't depict any valueble informations after the label encoding.

So, we went with One Hot encoding after few trials.
Below we can see how the encoding didn't help in understanding the correlation of the data
"""

NDF=pd.read_csv('/content/Crop_production.csv')

NDF.info()

DF1=NDF.drop(NDF.columns[[0,1,2,10,11]], axis=1, inplace=False)

DF1.info()

from sklearn.preprocessing import LabelEncoder
crop_encoder = LabelEncoder()
DF1['Crop_Encoded'] = crop_encoder.fit_transform(DF1['Crop'])
DF1=DF1.drop(DF1.columns[[0]], axis=1, inplace=False)

"""## **Now we check the CORRELATION of the variables of the dataset**

we do that by plotting the correlation matirx
"""

NDF.head()

NDF.keys()

matrix = DF1.corr()

ax = plt.axes()
sns.heatmap(matrix, cmap="coolwarm", annot=True)

ax.set_title('Correlation matrix')
plt.show()
#plotting correlation matrix

"""* If the value is -1, it means that there is a perfect negative correlation.



* If the value is 0, it means that there is no correlation between the two variables.

* If the value is 1, it means that there is a perfect positive correlation.

**now let's plot one without the annotations, just for aesthetics**
"""

sns.heatmap(matrix, cmap="coolwarm", annot=False)
plt.title('Correlation matrix without the annotations, just for the aesthetics')

plt.show()

"""**By the correlation matrix we can find that the Variables are not Correlated well, when the correlation values are near 0, it means that there is not much correlation between the variables.**

### **Now we split the processed dataset into X(train and test) and y(train and test)** 📁 ➡ 📂 , 📂
"""

yy = finalOHDF.loc[:, ['Yield_ton_per_hec']]

yy.head()

XX = finalOHDF.drop(['Yield_ton_per_hec'], axis=1)

XX.columns

XX.shape

#Spliting dataset for training and testing
OXtrain, OXtest, Oytrain, Oytest = train_test_split(XX , yy, test_size=0.2, random_state=42)

"""

---



---

"""

cdf.head()

cdf.columns

finalOHDF.head()

finalLDF.head()

finalLDF.columns

yy = finalLDF.loc[:, ['Yield_ton_per_hec']]

yy.head()

XX = finalLDF.drop(['Yield_ton_per_hec'], axis=1)

XX.columns

#Spliting dataset for training and testing
LXtrain, LXtest, Lytrain, Lytest = train_test_split(XX , yy, test_size=0.2, random_state=42)

"""## **Another Data Dictonary**

SLFXTr: scaled label final x_train

SLFXTe: scaled label final x_test

SLFYTr: scaled label final y_train

SLFYTe: scaled label final y_test

SOFXTr: scaled OH final x_train

SOFXTe: scaled OH final x_test

SOFYTr: scaled OH final y_train

SOFYTe: scaled OH final y_test

OXtrain, OXtest, Oytrain, Oytest :One hot encoded, log transformed

LXtrain, LXtest, Lytrain, Lytest :Label encoded, log transformed

---


we are doing this to keep track of the variable names

#**Feature Scaling** 📏

###Now we will do Feature Scaling on the variables to get their values into a easy to work range

there are many types of Scalling algorithms like:


* MinMax Scaler

* Standard Scaler

* MaxAbsScaler

* Robust Scaler

* Quantile Transformer Scaler

* Power Transformer Scaler

we will go with **Standard Scaler** because it can be used on all types of data.

the reason why we chose to do scaling even after log transformations is because we found this below information in a Reddit post.

    tree-based models are generally less sensitive to feature scaling.
    However, in some cases, scaling can still improve performance.
    It's often recommended to try both approaches (with and without scaling) and see which one performs better on your specific dataset.
    
so we thought it wouldn't hurt to try 😁
"""

columns_to_scale = ['nitrogen', 'phosphorus', 'potassium', 'pH', 'rainfall', 'temperature']
OXtrain[columns_to_scale].head()

from sklearn.preprocessing import StandardScaler
columns_to_scale = ['nitrogen', 'phosphorus', 'potassium', 'pH', 'rainfall', 'temperature']

# Create StandardScaler object
scaler = StandardScaler()

# Fit scaler to selected columns


# Transform selected columns
SOFXTr= OXtrain.copy(deep=True)  # Create a copy of the DataFrame to avoid modifying the original
SOFXTe= OXtest.copy(deep=True)

SLFXTr= LXtrain.copy(deep=True)  # Create a copy of the DataFrame to avoid modifying the original
SLFXTe= LXtest.copy(deep=True)

scaler.fit(SOFXTr[columns_to_scale])
#we wont be fitting the scaler to SLFXTr because the variables we chose to scale are same in both SOFXTr and SLFXTr


SOFXTr[columns_to_scale] = scaler.transform(SOFXTr[columns_to_scale]) #one hot encoded
SOFXTe[columns_to_scale] = scaler.transform(SOFXTe[columns_to_scale])


SLFXTr[columns_to_scale] = scaler.transform(SLFXTr[columns_to_scale]) #label encoded
SLFXTe[columns_to_scale] = scaler.transform(SLFXTe[columns_to_scale])

# Print scaled DataFrame
print("Original DataFrame:")
OXtrain.head()

print("\nScaled DataFrame:")
SOFXTr.head()

SOFYTr= Oytrain.copy(deep=True)
yscaler = StandardScaler()
data_to_scale = Oytrain[['Yield_ton_per_hec']].values.reshape(-1, 1)  # Reshape to 2D array
print(type(data_to_scale))
print(data_to_scale)
scaled_data = yscaler.fit_transform(data_to_scale)
SOFYTr['Yield_ton_per_hec'] = scaled_data.squeeze()  # Remove extra dimension

yscaler = StandardScaler()

SOFYTr= Oytrain.copy(deep=True)  # Create a copy of the DataFrame to avoid modifying the original
SOFYTe= Oytest.copy(deep=True)

SLFYTr= Lytrain.copy(deep=True)
SLFYTe= Lytest.copy(deep=True)

data_SOFYTr = SOFYTr[['Yield_ton_per_hec']].values.reshape(-1, 1)
data_SOFYTe = SOFYTe[['Yield_ton_per_hec']].values.reshape(-1, 1)
data_SLFYTr = SLFYTr[['Yield_ton_per_hec']].values.reshape(-1, 1)
data_SLFYTe = SLFYTe[['Yield_ton_per_hec']].values.reshape(-1, 1)

yscaler.fit_transform(data_SOFYTr)
#we won't do a fiting on the SLFYTr seperately because it is the same as SOFYTr

scaledSOFYTr = yscaler.transform(data_SOFYTr)
scaledSOFYTe = yscaler.transform(data_SOFYTe)

scaledSLFYTr = yscaler.transform(data_SLFYTr)
scaledSLFYTe = yscaler.transform(data_SLFYTe)

#.squeeze() removes any extra dimensions from scaled_data,
#ensuring it's a 1D array suitable for assigning to a DataFrame column.
SOFYTr['Yield_ton_per_hec'] = scaledSOFYTr.squeeze()
SOFYTe['Yield_ton_per_hec'] = scaledSOFYTe.squeeze()
SLFYTr['Yield_ton_per_hec'] = scaledSLFYTr.squeeze()
SLFYTe['Yield_ton_per_hec'] = scaledSLFYTe.squeeze()

print("Original DataFrame:")
print(type(Oytrain))
Oytrain.head()

print("\nScaled DataFrame:")
print(type(SOFYTr))
SOFYTr.head()

"""## **Now, we can continue the model building process.**
 So we will start with **linear regression** model and then move over to more complex models like **RandomForest** and **XGBoost**.

We will build the following models

*  **Linear Regression**

*  **Decision Tree**

*  **Random Forest**

*  **XGBoost**
"""

if RUN ALL will stop here.
this cell will raise an error and stop the execution. now only run the models that you want

"""### **linear regression**

with scaled, label encoded and log transformed data.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Assuming you have SLFXTr, SLFXTe, SLFYTr, and SLFYTe

# Initialize linear regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(SLFXTr, SLFYTr)

# Predict on the testing data
ypred = model.predict(SLFXTe)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(SLFYTe, ypred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate R-squared
r_squared = r2_score(SLFYTe, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with scaled, one hot encoded and log transformed data.

"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Assuming you have SOFXTr, SOFXTe, SOFYTr, and SOFYTe

# Initialize linear regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(SOFXTr, SOFYTr)

# Predict on the testing data
ypred = model.predict(SOFXTe)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(SOFYTe, ypred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate R-squared
r_squared = r2_score(SOFYTe, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with one hot encoded and log transformed

"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Assuming you have OXtrain, OXtest, Oytrain, and Oytest

# Initialize linear regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(OXtrain, Oytrain)

# Predict on the testing data
ypred = model.predict(OXtest)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(Oytest, ypred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate R-squared
r_squared = r2_score(Oytest, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with label encoded and log transformed"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np


# Initialize linear regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(LXtrain, Lytrain)

# Predict on the testing data
ypred = model.predict(LXtest)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(Lytest, ypred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate R-squared
r_squared = r2_score(Lytest, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""### now **decision trees**
**hyperparameters**

max_depth: Maximum depth of the decision
tree.

min_samples_split: The minimum number of samples required to split an internal node.

min_samples_leaf: the minimum number of samples required to be at a leaf node.

verbose=0: Silent mode - no output during training.

verbose=1: Progress bar mode - displays a progress bar with training and validation metrics (default).

verbose=2: One line per epoch - shows a summary of training and validation metrics after each epoch.

with scaled, label encoded and log transformed data.
"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

# Define the hyperparameters to tune
param_grid = {
    'max_depth': [ 20,30,40,60],
    'min_samples_split': [3,10,50,25],
    'min_samples_leaf': [1, 2, 4,8],
}

# Initialize Decision Tree Regressor
model = DecisionTreeRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=2)

# Fit the grid search to the data
grid_search.fit(SLFXTr, SLFYTr)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(SLFXTe)

# Calculate metrics
mse = mean_squared_error(SLFYTe, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(SLFYTe, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with scaled, one hot encoded and log transformed data.

"""

# Define the hyperparameters to tune
param_grid = {
    'max_depth': [ 20,30,40,60],
    'min_samples_split': [3,10,50,25],
    'min_samples_leaf': [1, 2, 4,8],
}

# Initialize Decision Tree Regressor
model = DecisionTreeRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=2)

# Fit the grid search to the data
grid_search.fit(SOFXTr, SOFYTr)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(SOFXTe)

# Calculate metrics
mse = mean_squared_error(SOFYTe, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(SOFYTe, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with one hot encoded and log transformed

"""

# Define the hyperparameters to tune
param_grid = {
    'max_depth': [ 20,30,40,60],
    'min_samples_split': [3,10,50,25],
    'min_samples_leaf': [1, 2, 4,8],
}

# Initialize Decision Tree Regressor
model = DecisionTreeRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=2)

# Fit the grid search to the data
grid_search.fit(OXtrain, Oytrain)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(OXtest)

# Calculate metrics
mse = mean_squared_error(Oytest, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(Oytest, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with label encoded and log transformed"""

param_grid = {
    'max_depth': [ 20,30,40,60],
    'min_samples_split': [3,10,50,25],
    'min_samples_leaf': [1, 2, 4,8],
}

# Initialize Decision Tree Regressor
model = DecisionTreeRegressor(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=2)

# Fit the grid search to the data
grid_search.fit(LXtrain, Lytrain)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(LXtest)

# Calculate metrics
mse = mean_squared_error(Lytest, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(Lytest, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""### now **random forest**
**hyperparameters**

n_estimators: Number of decision trees in the random forest.

max_depth: Maximum depth of each decision tree.

min_samples_split: Minimum number of samples required to split an internal node.

min_samples_leaf: Minimum number of samples required to be at a leaf node.
"""

flatSLFYTr=SLFYTr.values.ravel()
flatSOFYTr=SOFYTr.values.ravel()
flatOytrain=Oytrain.values.ravel()
flatLytrain=Lytrain.values.ravel()

"""with scaled, label encoded and log transformed data."""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# Define the parameter grid
param_grid = {
    "n_estimators": [100, 500],
    "max_depth": [5, 10, 15],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 3]
}

# Create the random forest regression model
model = RandomForestRegressor(random_state=42)

# Perform grid search hyperparameter tuning
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error',verbose=2)
grid_search.fit(SLFXTr, flatSLFYTr)

# Print the best hyperparameters and score
print("Best hyperparameters:", grid_search.best_params_)

# Make predictions on the test set
ytest_pred = grid_search.best_estimator_.predict(SLFXTe)

# Calculate the MSE, RMSE, and R-squared score
mse = mean_squared_error(SLFYTe, ytest_pred)
rmse = np.sqrt(mse)
r2 = r2_score(SLFYTe, ytest_pred)

print("Mean squared error (MSE):", mse)
print("Root mean squared error (RMSE):", rmse)
print("R-squared score:", r2)

"""with scaled, one hot encoded and log transformed data.

"""

# Define the parameter grid
param_grid = {
    "n_estimators": [100, 500],
    "max_depth": [5, 10, 15],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 3]
}

# Create the random forest regression model
model = RandomForestRegressor(random_state=42)

# Perform grid search hyperparameter tuning
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error',verbose=1)
grid_search.fit(SOFXTr, flatSOFYTr)

# Print the best hyperparameters and score
print("Best hyperparameters:", grid_search.best_params_)

# Make predictions on the test set
ytest_pred = grid_search.best_estimator_.predict(SOFXTe)

# Calculate the MSE, RMSE, and R-squared score
mse = mean_squared_error(SOFYTe, ytest_pred)
rmse = np.sqrt(mse)
r2 = r2_score(SOFYTe, ytest_pred)

print("Mean squared error (MSE):", mse)
print("Root mean squared error (RMSE):", rmse)
print("R-squared score:", r2)

"""with one hot encoded and log transformed

"""

param_grid = {
    "n_estimators": [100, 500],
    "max_depth": [5, 10, 15],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 3]
}

# Create the random forest regression model
model = RandomForestRegressor(random_state=42)

# Perform grid search hyperparameter tuning
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error',verbose=1)
grid_search.fit(OXtrain, flatOytrain)

# Print the best hyperparameters and score
print("Best hyperparameters:", grid_search.best_params_)

# Make predictions on the test set
ytest_pred = grid_search.best_estimator_.predict(OXtest)

# Calculate the MSE, RMSE, and R-squared score
mse = mean_squared_error(Oytest, ytest_pred)
rmse = np.sqrt(mse)
r2 = r2_score(Oytest, ytest_pred)

print("Mean squared error (MSE):", mse)
print("Root mean squared error (RMSE):", rmse)
print("R-squared score:", r2)

"""with label encoded and log transformed"""

# Define the parameter grid
param_grid = {
    "n_estimators": [100, 500],
    "max_depth": [5, 10, 15],
    "min_samples_split": [2, 4, 6],
    "min_samples_leaf": [1, 2, 3]
}

# Create the random forest regression model
model = RandomForestRegressor(random_state=42)

# Perform grid search hyperparameter tuning
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error',verbose=1)
grid_search.fit(LXtrain, flatLytrain)

# Print the best hyperparameters and score
print("Best hyperparameters:", grid_search.best_params_)
# Make predictions on the test set
ytest_pred = grid_search.best_estimator_.predict(LXtest)

# Calculate the MSE, RMSE, and R-squared score
mse = mean_squared_error(Lytest, ytest_pred)
rmse = np.sqrt(mse)
r2 = r2_score(Lytest, ytest_pred)

print("Mean squared error (MSE):", mse)
print("Root mean squared error (RMSE):", rmse)
print("R-squared score:", r2)

"""### now **xgboost**

hyperparameters:

learning_rate: Controls the step size for each iteration.

max_depth: Maximum depth of each tree.

min_child_weight: Minimum sum of instance weight in a child.

gamma: Minimum loss reduction to make a split.

subsample: Fraction of samples used for each tree.

colsample_bytree: Fraction of features used for each tree.

n_estimators: Number of trees in the model.



---



but for the sake of simplicity we will go with

learning rate, max depth, min child weight and n estimators
"""

import xgboost as xgb

"""with scaled, label encoded and log transformed data.

"""

# Define the hyperparameters to tune
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'n_estimators': [100, 200, 300],
}

# Initialize XGBoost Regressor
model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search to the data
grid_search.fit(SLFXTr, SLFYTr)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(SLFXTe)

# Calculate metrics
mse = mean_squared_error(SLFYTe, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(SLFYTe, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with scaled, one hot encoded and log transformed data.

"""

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters to tune
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'n_estimators': [100, 200, 300],
}

# Initialize XGBoost Regressor
model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search to the data
grid_search.fit(SOFXTr, SOFYTr)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(SOFXTe)

# Calculate metrics
mse = mean_squared_error(SOFYTe, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(SOFYTe, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with one hot encoded and log transformed

"""

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters to tune
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'n_estimators': [100, 200, 300],
}

# Initialize XGBoost Regressor
model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search to the data
grid_search.fit(OXtrain, Oytrain)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(OXtest)

# Calculate metrics
mse = mean_squared_error(Oytest, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(Oytest, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)

"""with label encoded and log transformed"""

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters to tune
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'n_estimators': [100, 200, 300],
}

# Initialize XGBoost Regressor
model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error', verbose=1)

# Fit the grid search to the data
grid_search.fit(LXtrain, Lytrain)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best model for prediction
best_model = grid_search.best_estimator_
ypred = best_model.predict(LXtest)

# Calculate metrics
mse = mean_squared_error(Lytest, ypred)
rmse = np.sqrt(mse)
r_squared = r2_score(Lytest, ypred)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared:", r_squared)